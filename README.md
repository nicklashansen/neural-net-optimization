# Optimization for Deep Learning

This repository will contain implementations of various popular optimization algorithms for deep learning, including SGD, Adam, AdamW and RAdam. Work in progress!

_____

## Related papers

Material in this repository has been developed as part of a special course / study. This is the tentative list of papers that we discuss:

[An Overview of Gradient Descent Optimization Algorithms](https://arxiv.org/abs/1609.04747)

[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

[On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237)

[Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)

[On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265v1)

[Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610)

[Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization](https://arxiv.org/abs/1903.02140v1)

[Optimization Methods for Large-Scale Machine Learning)(https://arxiv.org/abs/1606.04838)
